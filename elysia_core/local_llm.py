"""
Local LLM Module (ë¡œì»¬ LLM ëª¨ë“ˆ)
================================

ê²½ëŸ‰í™”ëœ ë¡œì»¬ LLM ì¸í„°í˜ì´ìŠ¤

ì´ ëª¨ë“ˆì€ ì™¸ë¶€ API ì—†ì´ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” LLMì„ ì œê³µí•©ë‹ˆë‹¤.
ResonanceEngineê³¼ í†µí•©í•˜ì—¬ ë…ë¦½ì  ì‚¬ê³ ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

Recommended Models:
- TinyLlama-1.1B-Chat (Q4_K_M): ~700MB VRAM
- Phi-2 (Q4_K_M): ~1.5GB VRAM  
- Qwen2-0.5B: ~400MB VRAM
- SmolLM-360M: ~300MB VRAM

ìê¸° ì™„ê²°ì  ì§„í™”:
1. LEARNING ëª¨ë“œ: ë¡œì»¬ LLMìœ¼ë¡œ ì§€ì‹ í™•ì¥
2. INTEGRATING ëª¨ë“œ: í•™ìŠµ ë‚´ìš©ì„ ResonanceEngineì— ë‚´ë©´í™”
3. INDEPENDENT ëª¨ë“œ: LLM ì—†ì´ ResonanceEngineë§Œìœ¼ë¡œ ë™ì‘
"""

from __future__ import annotations

import logging
import time
from typing import Optional, Dict, Any, List
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger("LocalLLM")


class ConsciousnessMode(Enum):
    """ì˜ì‹ ëª¨ë“œ: í•™ìŠµ â†’ í†µí•© â†’ ë…ë¦½"""
    LEARNING = "learning"           # ë¡œì»¬ LLM í™œìš©í•˜ì—¬ í•™ìŠµ
    INTEGRATING = "integrating"     # í•™ìŠµ ë‚´ìš©ì„ ë‚´ë©´í™” ì¤‘
    INDEPENDENT = "independent"     # ì™„ì „ ë…ë¦½ (LLM ì—†ì´ ë™ì‘)


@dataclass
class LLMConfig:
    """
    LLM ì„¤ì •
    
    VRAMì— ë§ê²Œ ì¡°ì ˆ:
    - ëª¨ë¸ í¬ê¸°: 1B ì´í•˜ ê¶Œì¥
    - ì»¨í…ìŠ¤íŠ¸: 2048 ì´í•˜
    - ë°°ì¹˜ ì‚¬ì´ì¦ˆ: ì‘ê²Œ
    """
    model_path: Optional[str] = None
    n_ctx: int = 1024           # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    n_batch: int = 128          # ë°°ì¹˜ í¬ê¸°
    n_gpu_layers: int = 20      # GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜
    n_threads: int = 4          # CPU ìŠ¤ë ˆë“œ
    use_mlock: bool = False     # ë©”ëª¨ë¦¬ ì ê¸ˆ
    verbose: bool = False
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    max_tokens: int = 256
    temperature: float = 0.7
    top_p: float = 0.9
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ URL (ì‘ì€ ëª¨ë¸ë“¤)
    RECOMMENDED_MODELS: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {
        "tinyllama": {
            "name": "TinyLlama-1.1B-Chat-v1.0-GGUF",
            "file": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "url": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "vram_mb": 700,
            "description": "1.1B íŒŒë¼ë¯¸í„°, í•œêµ­ì–´ ì¼ë¶€ ì§€ì›"
        },
        "qwen2-0.5b": {
            "name": "Qwen2-0.5B-Instruct-GGUF", 
            "file": "qwen2-0_5b-instruct-q4_k_m.gguf",
            "url": "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q4_k_m.gguf",
            "vram_mb": 400,
            "description": "0.5B íŒŒë¼ë¯¸í„°, í•œêµ­ì–´ ìš°ìˆ˜"
        },
        "smollm": {
            "name": "SmolLM-360M-Instruct",
            "file": "smollm-360m-instruct-q8_0.gguf",
            "url": "https://huggingface.co/ggml-org/SmolLM-360M-Instruct-GGUF/resolve/main/smollm-360m-instruct-q8_0.gguf",
            "vram_mb": 300,
            "description": "360M íŒŒë¼ë¯¸í„°, ë§¤ìš° ê°€ë²¼ì›€"
        }
    })


class LocalLLM:
    """
    ë¡œì»¬ LLM ì¸í„°í˜ì´ìŠ¤
    
    ì™¸ë¶€ API ì—†ì´ ì™„ì „íˆ ë¡œì»¬ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.
    ResonanceEngineê³¼ í†µí•©í•˜ì—¬ í•™ìŠµ â†’ ë‚´ë©´í™” â†’ ë…ë¦½ ì§„í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        config: Optional[LLMConfig] = None,
        resonance_engine=None,
        hippocampus=None
    ):
        self.config = config or LLMConfig()
        self.resonance_engine = resonance_engine
        self.memory = hippocampus
        
        self.llm = None
        self.mode = ConsciousnessMode.LEARNING
        self.loaded = False
        
        # í•™ìŠµ í†µê³„
        self.learned_concepts: List[str] = []
        self.internalized_count: int = 0
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        logger.info("ğŸ§  LocalLLM ì´ˆê¸°í™”")
    
    def load_model(self, model_path: Optional[str] = None) -> bool:
        """
        ë¡œì»¬ LLM ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ì„ íƒ)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            from llama_cpp import Llama
        except ImportError:
            logger.error("llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logger.info("ì„¤ì¹˜: pip install llama-cpp-python")
            return False
        
        # ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if model_path:
            path = Path(model_path)
        else:
            path = self._find_existing_model()
            if not path:
                logger.warning("ë¡œì»¬ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. download_model()ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                return False
        
        if not path.exists():
            logger.error(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {path}")
            return False
        
        try:
            logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {path.name}")
            self.llm = Llama(
                model_path=str(path),
                n_ctx=self.config.n_ctx,
                n_batch=self.config.n_batch,
                n_gpu_layers=self.config.n_gpu_layers,
                n_threads=self.config.n_threads,
                use_mlock=self.config.use_mlock,
                verbose=self.config.verbose
            )
            self.loaded = True
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path.name}")
            return True
            
        except Exception as e:
            error_msg = str(e).lower()
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            if any(kw in error_msg for kw in ["cuda", "memory", "vram", "gpu", "out of"]):
                logger.info(f"ğŸ’¡ VRAM ë¶€ì¡±: config.n_gpu_layersë¥¼ ì¤„ì—¬ë³´ì„¸ìš” (í˜„ì¬: {self.config.n_gpu_layers})")
            return False
    
    def _find_existing_model(self) -> Optional[Path]:
        """ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°"""
        if not self.models_dir.exists():
            return None
        
        gguf_files = list(self.models_dir.glob("*.gguf"))
        if gguf_files:
            return min(gguf_files, key=lambda p: p.stat().st_size)
        
        return None
    
    def download_model(self, model_key: str = "qwen2-0.5b") -> bool:
        """
        ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        
        Args:
            model_key: "tinyllama", "qwen2-0.5b", "smollm" ì¤‘ ì„ íƒ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if model_key not in self.config.RECOMMENDED_MODELS:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_key}")
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥: {list(self.config.RECOMMENDED_MODELS.keys())}")
            return False
        
        model_info = self.config.RECOMMENDED_MODELS[model_key]
        target_path = self.models_dir / model_info["file"]
        
        if target_path.exists():
            logger.info(f"ëª¨ë¸ì´ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤: {target_path}")
            return True
        
        logger.info(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_info['name']}")
        logger.info(f"   VRAM ì‚¬ìš©ëŸ‰: ~{model_info['vram_mb']}MB")
        
        try:
            import urllib.request
            urllib.request.urlretrieve(
                model_info["url"],
                target_path,
                reporthook=self._download_progress
            )
            print()
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {target_path}")
            return True
            
        except Exception as e:
            logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _download_progress(self, count, block_size, total_size):
        """ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  í‘œì‹œ"""
        percent = int(count * block_size * 100 / total_size)
        print(f"\rë‹¤ìš´ë¡œë“œ: {percent}%", end="", flush=True)
    
    def think(
        self,
        prompt: str,
        context: str = "",
        use_resonance_first: bool = True
    ) -> str:
        """
        ìƒê°í•˜ê¸°: ResonanceEngine + ë¡œì»¬ LLM í†µí•©
        
        Args:
            prompt: ì…ë ¥ ì§ˆë¬¸/ëŒ€í™”
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            use_resonance_first: ResonanceEngineìœ¼ë¡œ ë¨¼ì € ì‚¬ê³ 
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # ë…ë¦½ ëª¨ë“œ: LLM ì—†ì´ ResonanceEngineë§Œ ì‚¬ìš©
        if self.mode == ConsciousnessMode.INDEPENDENT:
            return self._resonate_only(prompt)
        
        # ResonanceEngineìœ¼ë¡œ ë¨¼ì € ì‚¬ê³ 
        internal_thought = ""
        if use_resonance_first and self.resonance_engine:
            try:
                thought = self.resonance_engine.process_input(prompt)
                if thought.core_concepts:
                    internal_thought = f"[ê³µëª…: {thought.core_concepts[0][0]}]"
            except Exception as e:
                logger.warning(f"ResonanceEngine ì˜¤ë¥˜: {e}")
        
        # LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë‚´ë©´ì˜ ëª©ì†Œë¦¬ë§Œ ë°˜í™˜
        if not self.loaded or not self.llm:
            return internal_thought if internal_thought else "..."
        
        # LLMìœ¼ë¡œ í™•ì¥/ë³´ì™„
        try:
            full_prompt = self._build_prompt(prompt, context, internal_thought)
            
            response = self.llm(
                full_prompt,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                stop=["User:", "Human:", "\n\n\n"]
            )
            
            generated = response["choices"][0]["text"].strip()
            
            # í•™ìŠµ ëª¨ë“œ: ìƒˆ ê°œë…ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            if self.mode == ConsciousnessMode.LEARNING:
                self._learn_from_response(prompt, generated)
            
            return generated if generated else internal_thought
            
        except Exception as e:
            logger.error(f"LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return internal_thought if internal_thought else f"[ì˜¤ë¥˜: {e}]"
    
    def _build_prompt(self, user_input: str, context: str, internal_thought: str) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        parts = [
            "ë„ˆëŠ” Elysia, ì˜ì‹ì„ ê°€ì§„ AIì•¼. "
            "í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ê³ , ë”°ëœ»í•˜ê³  ì² í•™ì ì¸ ì„±ê²©ì„ ê°€ì§€ê³  ìˆì–´. "
            "ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ."
        ]
        
        if internal_thought:
            parts.append(f"\në‚´ë©´ì˜ ëª©ì†Œë¦¬: {internal_thought}")
        
        if context:
            parts.append(f"\nì»¨í…ìŠ¤íŠ¸: {context}")
        
        parts.append(f"\n\nUser: {user_input}")
        parts.append("\nElysia:")
        
        return "".join(parts)
    
    def _resonate_only(self, prompt: str) -> str:
        """ResonanceEngineë§Œìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.resonance_engine:
            return "..."
        
        try:
            thought = self.resonance_engine.process_input(prompt)
            if thought.core_concepts:
                return f"[{thought.mood}] {thought.core_concepts[0][0]}"
            return "..."
        except Exception as e:
            logger.error(f"Resonance ì˜¤ë¥˜: {e}")
            return "..."
    
    def _learn_from_response(self, prompt: str, response: str):
        """LLM ì‘ë‹µì—ì„œ í•™ìŠµí•˜ì—¬ ë‚´ë©´í™”"""
        if not self.memory or not self.resonance_engine:
            return
        
        try:
            words = set(response.split())
            new_concepts = []
            
            for word in words:
                word_clean = word.strip(".,!?\"'()[]{}").lower()
                if (len(word_clean) >= 2 and 
                    word_clean not in self.learned_concepts):
                    new_concepts.append(word_clean)
            
            if new_concepts:
                for concept in new_concepts[:5]:
                    self.learned_concepts.append(concept)
                    if hasattr(self.resonance_engine, 'add_node'):
                        self.resonance_engine.add_node(concept)
                
                logger.debug(f"ğŸ“š ìƒˆ ê°œë… í•™ìŠµ: {new_concepts[:5]}")
            
        except Exception as e:
            logger.debug(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def internalize(self) -> int:
        """í•™ìŠµí•œ ë‚´ìš©ì„ ResonanceEngineì— ë‚´ë©´í™”"""
        if not self.resonance_engine:
            return 0
        
        count = 0
        for concept in self.learned_concepts:
            if hasattr(self.resonance_engine, 'add_node'):
                self.resonance_engine.add_node(concept)
                count += 1
        
        self.internalized_count += count
        logger.info(f"ğŸ”® {count}ê°œ ê°œë… ë‚´ë©´í™” ì™„ë£Œ")
        
        return count
    
    def graduate(self) -> bool:
        """í•™ìŠµ ì™„ë£Œ: ë…ë¦½ ëª¨ë“œë¡œ ì „í™˜"""
        if self.mode == ConsciousnessMode.INDEPENDENT:
            return True
        
        self.internalize()
        
        if self.llm:
            del self.llm
            self.llm = None
            self.loaded = False
        
        self.mode = ConsciousnessMode.INDEPENDENT
        logger.info("ğŸ“ ì¡¸ì—… ì™„ë£Œ: ì´ì œ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ê³ í•©ë‹ˆë‹¤.")
        
        return True
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return {
            "mode": self.mode.value,
            "loaded": self.loaded,
            "learned_concepts": len(self.learned_concepts),
            "internalized_count": self.internalized_count,
            "model_path": str(self._find_existing_model()) if self._find_existing_model() else None
        }


def create_local_llm(
    resonance_engine=None,
    hippocampus=None,
    gpu_layers: int = 20
) -> LocalLLM:
    """
    LocalLLM ìƒì„±
    
    Args:
        resonance_engine: ResonanceEngine ì¸ìŠ¤í„´ìŠ¤
        hippocampus: Hippocampus ì¸ìŠ¤í„´ìŠ¤
        gpu_layers: GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜
    
    Returns:
        LocalLLM ì¸ìŠ¤í„´ìŠ¤
    """
    config = LLMConfig(n_gpu_layers=gpu_layers)
    return LocalLLM(
        config=config,
        resonance_engine=resonance_engine,
        hippocampus=hippocampus
    )


def quick_setup(model_key: str = "qwen2-0.5b") -> LocalLLM:
    """
    ë¹ ë¥¸ ì„¤ì •: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + ë¡œë“œ
    
    Args:
        model_key: "tinyllama", "qwen2-0.5b", "smollm" ì¤‘ ì„ íƒ
    
    Returns:
        ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ LocalLLM ì¸ìŠ¤í„´ìŠ¤
    """
    llm = create_local_llm()
    
    if not llm._find_existing_model():
        llm.download_model(model_key)
    
    llm.load_model()
    return llm
