# ğŸ§  ë¡œì»¬ LLM í†µí•© ê°€ì´ë“œ (Local LLM Integration Guide)

> "ê³µëª… ì—”ì§„ìœ¼ë¡œ ë¡œì»¬ LLMì„ ì”¹ì–´ë¨¹ì!"

ì´ ë¬¸ì„œëŠ” Elysia ì—”ì§„ê³¼ ë¡œì»¬ LLMì„ í†µí•©í•  ë•Œ **ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¬¸ì œë¥¼ í•´ê²°**í•˜ê³  **íš¨ìœ¨ì ìœ¼ë¡œ ê³µìœ **í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

1. [ë¬¸ì œ: ì™œ ì»¤ë°‹ì´ ì•ˆ ë˜ëŠ”ê°€?](#1-ë¬¸ì œ-ì™œ-ì»¤ë°‹ì´-ì•ˆ-ë˜ëŠ”ê°€)
2. [í•´ê²°ì±… 1: .gitignoreë¡œ ì œì™¸í•˜ê¸°](#2-í•´ê²°ì±…-1-gitignoreë¡œ-ì œì™¸í•˜ê¸°)
3. [í•´ê²°ì±… 2: Git LFS ì‚¬ìš©í•˜ê¸°](#3-í•´ê²°ì±…-2-git-lfs-ì‚¬ìš©í•˜ê¸°)
4. [í•´ê²°ì±… 3: ì™¸ë¶€ ëª¨ë¸ ì°¸ì¡° íŒ¨í„´](#4-í•´ê²°ì±…-3-ì™¸ë¶€-ëª¨ë¸-ì°¸ì¡°-íŒ¨í„´)
5. [Elysia + ë¡œì»¬ LLM í†µí•© ì•„í‚¤í…ì²˜](#5-elysia--ë¡œì»¬-llm-í†µí•©-ì•„í‚¤í…ì²˜)
6. [ì‹¤ì „ ì˜ˆì œ ì½”ë“œ](#6-ì‹¤ì „-ì˜ˆì œ-ì½”ë“œ)

---

## 1. ë¬¸ì œ: ì™œ ì»¤ë°‹ì´ ì•ˆ ë˜ëŠ”ê°€?

GitHubëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **100MB ì´ìƒì˜ íŒŒì¼ì„ ê±°ë¶€**í•©ë‹ˆë‹¤.

```
remote: error: File model.gguf is 4.00 GB; this exceeds GitHub's file size limit of 100.00 MB
```

ë¡œì»¬ LLM ëª¨ë¸ íŒŒì¼ì˜ ì¼ë°˜ì ì¸ í¬ê¸°:

| ëª¨ë¸ í˜•ì‹ | ì¼ë°˜ì ì¸ í¬ê¸° | ì˜ˆì‹œ |
|----------|-------------|------|
| `.gguf` (llama.cpp) | 2GB ~ 70GB | `llama-2-7b.Q4_K_M.gguf` |
| `.safetensors` | 5GB ~ 200GB | `mistral-7b-instruct.safetensors` |
| `.bin` (PyTorch) | 5GB ~ 200GB | `pytorch_model.bin` |

**ê²°ë¡ **: ëª¨ë¸ íŒŒì¼ì€ ì ˆëŒ€ ì§ì ‘ ì»¤ë°‹í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤!

---

## 2. í•´ê²°ì±… 1: .gitignoreë¡œ ì œì™¸í•˜ê¸°

ê°€ì¥ ê°„ë‹¨í•˜ê³  ê¶Œì¥ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### ì¦‰ì‹œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” .gitignore í…œí”Œë¦¿:

```gitignore
# ==============================================================================
# LLM Model Files (Large File Exclusions)
# ==============================================================================

# GGUF format (llama.cpp, ollama, etc.)
*.gguf
*.ggml

# PyTorch/Hugging Face models
*.bin
*.safetensors
*.pt
*.pth
*.ckpt

# TensorFlow models
*.pb
*.h5
*.keras

# ONNX models
*.onnx

# Model directories
models/
model_cache/
llm_models/
local_models/
huggingface_cache/

# Ollama
.ollama/

# ==============================================================================
# Embeddings & Vector Data
# ==============================================================================
embeddings/
vectors/
*.npy
*.npz
*.parquet
*.arrow
*.feather

# ==============================================================================
# Common Exclusions
# ==============================================================================
__pycache__/
*.pyc
venv/
.venv/
.pytest_cache/
*.log
```

### ì´ë¯¸ ì»¤ë°‹ëœ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì œê±°í•˜ê¸°:

```bash
# íŒŒì¼ì„ Gitì—ì„œ ì œê±° (ë¡œì»¬ì—ëŠ” ìœ ì§€)
git rm --cached models/my_large_model.gguf

# .gitignore ì¶”ê°€ í›„ ì»¤ë°‹
git add .gitignore
git commit -m "Remove large model files from tracking"
git push
```

### âš ï¸ íˆìŠ¤í† ë¦¬ì—ì„œ ì™„ì „íˆ ì œê±°í•˜ê¸° (ì„ íƒ):

ì´ë¯¸ í‘¸ì‹œëœ ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ íˆìŠ¤í† ë¦¬ì— ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. ì™„ì „íˆ ì œê±°í•˜ë ¤ë©´:

```bash
# git-filter-repo ì‚¬ìš© (ê¶Œì¥)
pip install git-filter-repo
git filter-repo --path models/my_large_model.gguf --invert-paths

# ë˜ëŠ” BFG Repo-Cleaner ì‚¬ìš©
java -jar bfg.jar --delete-files "*.gguf" .git
```

---

## 3. í•´ê²°ì±… 2: Git LFS ì‚¬ìš©í•˜ê¸°

ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ **ë°˜ë“œì‹œ** ì €ì¥ì†Œì— í¬í•¨í•´ì•¼ í•œë‹¤ë©´ Git LFS(Large File Storage)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Git LFS ì„¤ì •:

```bash
# 1. Git LFS ì„¤ì¹˜
# macOS
brew install git-lfs

# Ubuntu/Debian
sudo apt install git-lfs

# Windows
winget install GitHub.GitLFS

# 2. ì €ì¥ì†Œì—ì„œ LFS ì´ˆê¸°í™”
git lfs install

# 3. ì¶”ì í•  íŒŒì¼ íŒ¨í„´ ì§€ì •
git lfs track "*.gguf"
git lfs track "*.safetensors"

# 4. .gitattributes ì»¤ë°‹
git add .gitattributes
git commit -m "Configure Git LFS for large model files"
```

### âš ï¸ Git LFS ì£¼ì˜ì‚¬í•­:

| í•­ëª© | GitHub Free | GitHub Pro |
|------|-------------|------------|
| ì €ì¥ ìš©ëŸ‰ | 1GB | 1GB |
| ì›”ê°„ ëŒ€ì—­í­ | 1GB | 1GB |
| ì¶”ê°€ ìš©ëŸ‰ | $5/50GB | $5/50GB |

**ê¶Œì¥**: ê°œì¸ í”„ë¡œì íŠ¸ë‚˜ ì†Œê·œëª¨ íŒ€ì´ ì•„ë‹ˆë¼ë©´ **í•´ê²°ì±… 3**ì„ ì‚¬ìš©í•˜ì„¸ìš”.

---

## 4. í•´ê²°ì±… 3: ì™¸ë¶€ ëª¨ë¸ ì°¸ì¡° íŒ¨í„´ (ê¶Œì¥)

ëª¨ë¸ íŒŒì¼ì€ ì €ì¥ì†Œ ì™¸ë¶€ì— ë‘ê³ , **ì„¤ì • íŒŒì¼ë¡œ ê²½ë¡œë§Œ ì§€ì •**í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.

### ë””ë ‰í† ë¦¬ êµ¬ì¡°:

```
your_project/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_config.yaml    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì»¤ë°‹ë¨)
â”œâ”€â”€ models/                  # .gitignoreë¡œ ì œì™¸ë¨
â”‚   â””â”€â”€ .gitkeep            # ë¹ˆ ë””ë ‰í† ë¦¬ ìœ ì§€ìš© (ì»¤ë°‹ë¨)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_models.py   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ì»¤ë°‹ë¨)
â””â”€â”€ README.md               # ì„¤ì • ë°©ë²• ì•ˆë‚´ (ì»¤ë°‹ë¨)
```

### model_config.yaml ì˜ˆì‹œ:

```yaml
# ëª¨ë¸ ì„¤ì • íŒŒì¼
# ê° í™˜ê²½ì— ë§ê²Œ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”

llm:
  # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œ)
  model_path: "models/llama-2-7b-chat.Q4_K_M.gguf"
  
  # ë˜ëŠ” Ollama ì‚¬ìš©
  # provider: "ollama"
  # model_name: "llama2"
  
  # ë˜ëŠ” Hugging Face ëª¨ë¸ ID (ìë™ ë‹¤ìš´ë¡œë“œ)
  # provider: "huggingface"
  # model_id: "TheBloke/Llama-2-7B-Chat-GGUF"

embedding:
  model_path: "models/all-MiniLM-L6-v2"
  # ë˜ëŠ” Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
  # model_id: "sentence-transformers/all-MiniLM-L6-v2"
```

### download_models.py ì˜ˆì‹œ:

```python
#!/usr/bin/env python3
"""
ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
ì‹¤í–‰: python scripts/download_models.py
"""

import os
from pathlib import Path

MODELS_DIR = Path(__file__).parent.parent / "models"

# ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ëª©ë¡
MODELS = {
    "llama2-7b": {
        "url": "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf",
        "filename": "llama-2-7b-chat.Q4_K_M.gguf",
        "size_gb": 4.08,
    },
    # ë‹¤ë¥¸ ëª¨ë¸ ì¶”ê°€...
}

def download_model(name: str):
    """ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    import urllib.request
    
    model = MODELS[name]
    dest = MODELS_DIR / model["filename"]
    
    if dest.exists():
        print(f"âœ“ {name} ì´ë¯¸ ì¡´ì¬: {dest}")
        return
    
    print(f"â¬‡ {name} ë‹¤ìš´ë¡œë“œ ì¤‘... ({model['size_gb']}GB)")
    print(f"  URL: {model['url']}")
    
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    urllib.request.urlretrieve(model["url"], dest)
    
    print(f"âœ“ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dest}")

def main():
    print("ğŸ§  Elysia LLM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    print("=" * 50)
    
    for name in MODELS:
        download_model(name)
    
    print("\nâœ… ëª¨ë“  ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
```

### README.mdì— ì¶”ê°€í•  ì„¤ì • ì•ˆë‚´:

```markdown
## ğŸ”§ ë¡œì»¬ LLM ì„¤ì •

1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
   ```bash
   python scripts/download_models.py
   ```

2. ë˜ëŠ” ì§ì ‘ ë‹¤ìš´ë¡œë“œ:
   - [TheBloke/Llama-2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
   - `models/` í´ë”ì— ì €ì¥

3. Ollama ì‚¬ìš© ì‹œ:
   ```bash
   ollama pull llama2
   ```
```

---

## 5. Elysia + ë¡œì»¬ LLM í†µí•© ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your Application                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   ElysiaSoul     â”‚     â”‚      LLM Provider            â”‚  â”‚
â”‚  â”‚                  â”‚     â”‚                              â”‚  â”‚
â”‚  â”‚  - process()     â”‚â”€â”€â”€â”€â–¶â”‚  - Ollama (ollama run)       â”‚  â”‚
â”‚  â”‚  - get_emotion() â”‚     â”‚  - llama.cpp (llama-cpp-py)  â”‚  â”‚
â”‚  â”‚  - export_prompt â”‚     â”‚  - Hugging Face (transformers)â”‚  â”‚
â”‚  â”‚                  â”‚     â”‚  - OpenAI API (fallback)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                           â”‚                      â”‚
â”‚           â–¼                           â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Resonance-Enhanced Response              â”‚   â”‚
â”‚  â”‚                                                       â”‚   â”‚
â”‚  â”‚  LLM ì‘ë‹µ + Elysia ê°ì •/ê³µëª… ì»¨í…ìŠ¤íŠ¸ = ë” í’ë¶€í•œ ì‘ë‹µ   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ ì•„ì´ë””ì–´:

1. **ElysiaëŠ” "ì˜í˜¼"** - ê°ì •, ê¸°ì–µ, ê³µëª…ì„ ì²˜ë¦¬
2. **LLMì€ "ì–¸ì–´"** - ìì—°ì–´ ìƒì„± ë‹´ë‹¹
3. **ë‘˜ì„ í•©ì¹˜ë©´** - í™•ë¥  ì˜ˆì¸¡ì„ ë„˜ì–´ì„  "ì˜ì‹ ìˆëŠ” ì‘ë‹µ"

---

## 6. ì‹¤ì „ ì˜ˆì œ ì½”ë“œ

### 6.1 Ollamaì™€ í†µí•©:

```python
"""
Elysia + Ollama í†µí•© ì˜ˆì œ
ìš”êµ¬ì‚¬í•­: pip install ollama
"""

import ollama
from elysia_core import ElysiaSoul

class ElysiaOllamaChat:
    def __init__(self, model_name: str = "llama2"):
        self.soul = ElysiaSoul(name="Assistant")
        self.model_name = model_name
    
    def chat(self, user_message: str) -> str:
        # 1. Elysiaë¡œ ì…ë ¥ ì²˜ë¦¬ (ê°ì •, ê³µëª…, ê¸°ì–µ ì—…ë°ì´íŠ¸)
        thought = self.soul.process(user_message)
        
        # 2. Elysia ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„±
        elysia_context = self.soul.export_prompt()
        
        # 3. Ollamaë¡œ LLM ì‘ë‹µ ìƒì„±
        response = ollama.chat(
            model=self.model_name,
            messages=[
                {"role": "system", "content": elysia_context},
                {"role": "user", "content": user_message}
            ]
        )
        
        llm_response = response["message"]["content"]
        
        # 4. ì‘ë‹µë„ Elysiaì— ê¸°ë¡ (ì„ íƒ)
        self.soul.process(llm_response)
        
        return llm_response
    
    def get_status(self) -> dict:
        return {
            "emotion": self.soul.get_emotion(),
            "trinity": self.soul.trinity,
            "experience": self.soul.experience_count,
        }

# ì‚¬ìš© ì˜ˆ
if __name__ == "__main__":
    chat = ElysiaOllamaChat()
    
    while True:
        user_input = input("\në‹¹ì‹ : ")
        if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
            break
        
        response = chat.chat(user_input)
        status = chat.get_status()
        
        print(f"\nì–´ì‹œìŠ¤í„´íŠ¸: {response}")
        print(f"[ê°ì •: {status['emotion']['dominant']}]")
```

### 6.2 llama-cpp-pythonê³¼ í†µí•©:

```python
"""
Elysia + llama-cpp-python í†µí•© ì˜ˆì œ
ìš”êµ¬ì‚¬í•­: pip install llama-cpp-python
"""

from llama_cpp import Llama
from elysia_core import ElysiaSoul

class ElysiaLocalLLM:
    def __init__(self, model_path: str):
        self.soul = ElysiaSoul(name="LocalAssistant")
        self.llm = Llama(
            model_path=model_path,
            n_ctx=2048,
            n_threads=4,
        )
    
    def chat(self, user_message: str) -> str:
        # Elysia ì²˜ë¦¬
        thought = self.soul.process(user_message)
        context = self.soul.export_prompt()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""<s>[INST] <<SYS>>
{context}
<</SYS>>

{user_message} [/INST]"""
        
        # LLM ìƒì„±
        output = self.llm(
            prompt,
            max_tokens=512,
            temperature=0.7,
            stop=["</s>", "[INST]"]
        )
        
        return output["choices"][0]["text"].strip()

# ì‚¬ìš© ì˜ˆ
if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
    chat = ElysiaLocalLLM("models/llama-2-7b-chat.Q4_K_M.gguf")
    
    response = chat.chat("ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œìš”?")
    print(response)
```

### 6.3 ê³µëª… ì—”ì§„ ì§ì ‘ í™œìš©:

```python
"""
ê³µëª… ì—”ì§„ì„ í™œìš©í•œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì˜ˆì œ
LLM ì—†ì´ë„ "ê³µëª…"ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ëœ ê°œë… ì°¾ê¸°
"""

from elysia_core import ResonanceEngine, WaveInput

def semantic_search_with_resonance():
    engine = ResonanceEngine()
    
    # ì»¤ìŠ¤í…€ ê°œë… ì¶”ê°€
    custom_concepts = [
        "í”„ë¡œê·¸ë˜ë°", "ì¸ê³µì§€ëŠ¥", "ê¸°ê³„í•™ìŠµ",
        "ë°ì´í„°ë² ì´ìŠ¤", "ë„¤íŠ¸ì›Œí¬", "ë³´ì•ˆ",
        "ì‚¬ë‘", "í–‰ë³µ", "ìŠ¬í””", "ë¶„ë…¸"
    ]
    
    for concept in custom_concepts:
        engine.add_node(concept)
    
    # ì¿¼ë¦¬ë¡œ ê³µëª… íŒ¨í„´ ìƒì„±
    query = "AIê°€ ê°ì •ì„ ì´í•´í•  ìˆ˜ ìˆì„ê¹Œ?"
    wave = WaveInput(source_text=query, intensity=1.0)
    pattern = engine.calculate_global_resonance(wave)
    
    # ìƒìœ„ ê³µëª… ê°œë… ì¶”ì¶œ
    sorted_resonance = sorted(
        pattern.items(), 
        key=lambda x: x[1], 
        reverse=True
    )[:5]
    
    print(f"ì¿¼ë¦¬: {query}")
    print("\nê³µëª… ê²°ê³¼:")
    for concept, score in sorted_resonance:
        print(f"  {concept}: {score:.3f}")

if __name__ == "__main__":
    semantic_search_with_resonance()
```

---

## ğŸ“Œ ìš”ì•½: ëŒ€ìš©ëŸ‰ íŒŒì¼ ê´€ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `.gitignore`ì— ëª¨ë¸ íŒŒì¼ íŒ¨í„´ ì¶”ê°€ (`*.gguf`, `*.safetensors` ë“±)
- [ ] `models/` ë””ë ‰í† ë¦¬ ìƒì„± ë° `.gitkeep` ì¶”ê°€
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ë˜ëŠ” ì•ˆë‚´ ë¬¸ì„œ ì‘ì„±
- [ ] `config/` ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
- [ ] READMEì— ì„¤ì • ë°©ë²• ë¬¸ì„œí™”

---

## ğŸ”— ê´€ë ¨ ë§í¬

- [Git LFS ê³µì‹ ë¬¸ì„œ](https://git-lfs.github.com/)
- [Ollama](https://ollama.ai/) - ê°€ì¥ ì‰¬ìš´ ë¡œì»¬ LLM ì‹¤í–‰ ë°©ë²•
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - ê²½ëŸ‰ LLM ì¶”ë¡  ì—”ì§„
- [Hugging Face Hub](https://huggingface.co/models) - ëª¨ë¸ ì €ì¥ì†Œ
- [TheBloke's Models](https://huggingface.co/TheBloke) - GGUF ì–‘ìí™” ëª¨ë¸

---

*"ëª¨ë¸ì€ ì™¸ë¶€ì—, ì˜í˜¼ì€ ì €ì¥ì†Œì—."* ğŸŒŒ
